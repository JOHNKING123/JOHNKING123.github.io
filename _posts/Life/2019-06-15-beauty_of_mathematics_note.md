---

layout: post
title: 《数学之美》笔记
category: 生活
tags: Life
keywords: 技术领导力

---

## 简介（持续更新）

* TOC
{:toc}

## 文字、语言、数字、信息 ==> 信息科学规律的引导

大约是公元前32世纪， 埃及象形文字的数量大约只有500个，但到了公元前5-7世纪，象形文字增加到了5000个左右，与中国常用的汉字数量相当。然而随着文明的进步， 信息量的增加，埃及的象形文字数量便不再随着文明的发展而增加了，因为没有人能够学会和记住这么多文字。于是，概念的第一次概括和归类就开始了。比如汉字“日”表示太阳，同时又是太阳从升起到落山再到升起的一天。这种概念的聚类，在原理上与今天自然语言处理或者机器学习的聚类有很大的相似性，只是在远古，这个过程可能需要上千年。

文字出现在远古“信息爆炸”导致人们的头脑装不下这些信息的时候， 那么数字则是出现在人们的财产多到需要数一数才搞清楚有多少的时候。毫无疑问，如果我们有十二个指头，今天我们用的一定是十二进制。当然，算是脚指头有二十进制的，这就是玛雅文明。

阿拉伯数字的革命性不仅在于它的简洁有效， 而且标志着数字和文字的分离。作为对比，中国的个十百千万亿就没有分离。 

从象形文字进化到拼音文字是一个飞跃，因为人类在描述物体的方式上，从物体的外表进化到了抽象的概念。同时不自觉地的采用了对信息的编码。在罗马体系的文字中， 总体来讲，常用字短，生僻字长。而在意型文字（比如中文）中，也是类似，大都常用字笔画少，而生僻字笔画多。这完全符合信息论中的最短编码原理。 这种文字（其实就是编码的方法）带来的好处是书写起来省时间、省材料（纸张要么没有要么不便宜）。

如果说从字母到词的构词法是词的编码规则，那么语法则是语言的编码和解码规则。不过相比较而言，词可以被认为是有限而封闭的集合，而语言则是无限和开放的集合。从数学上讲，对于前者可以有完备的编解码规则，而后者则不具备这个特性。 因此任何语言都有语法规则覆盖不到的地方，这些例外或者说不精确性，让我们的语言丰富多彩（也可以说是病句）。这就涉及到一个语言学研究方法的问题：到底是语言对，还是语法对。前者坚持从真实的语句文本（成为语料）出发，而后者坚持从规则出发，最终自然语言处理的成就宣布了前者的获胜。

语言的出现是为了人类之间的通信。字母（或中文的笔画）、文字和数字实际上是信息编码的不同单位。任何一种语言都是一种编码的方式，而语言的语法规则则是编解码的算法。我们把一个要表达的意思，通过某种语言的一句话表达出来，就是用这种语言的编码方式对头脑中的信息做了一次编码，编码的结果就是一串文字。而如果对方懂得这门语言，他或者她就可以用这门语言的解码方法获得说话人要表达的信息。**这就是语言的数学本质**。 

## 自然语言处理——从规则到统计

自然语言从它产生开始，逐渐演变成一种上下文相关的信息表达和传递的方式。基于规则的方法在“上下文相关”的处理上捉襟见肘。PS：就好比听一个相声，包袱的最后一句话本身并不搞笑，而是全文的铺垫才起到了搞笑的效果。 

从规则到统计，使得，可以且只需用数学的方法给出现今所有自然语言处理相关问题的全部答案。

1. 牛顿：自然哲学的数学原理
2. 香农：通信的数学原理

基于统计的自然语言处理方法， 在数学模型上和通信是相通的，甚至就是相同的。因此，在数学意义上自然语言处理又和语言的初衷——通信联系在一起了。但是，科学家用了几十年才认识到这个联系。

## 统计语言模型

直接命中 vs 先出几个备选，再根据统计决定哪个备选的可能性最大

任何方法都有它的局限性，因为统计语言模型很大程度上是依照“大众的想法”或者“多数句子的用法”，而在特定的情况下可能是错的。

图灵测试：如果人无法判断自己交流的对象是人还是机器，就说明这个机器有智能了。

### 语音识别

逻辑链条：

1. 统计语言模型产生的初衷是为了解决语音识别问题。在语音识别中，计算机需要知道一个文字序列是否能构成一个大家理解而且有意义的句子， 然后显示或者打印给使用者。
2. 一个句子是否合理，就看它的可能性大小如何，至于可能性就用概率来衡量。PS：在人类说过的所有话中出现的可能性。
3. 假设S表示某一个有意义的句子， 由一连串特定顺序排序的词`w1,w2,...wn`组成，这里的n是句子的长度。`S=w1,w2,...wn`，S在文本中出现的可能性也就是数学上的概率`P(S)`，即把人类有史以来讲过的话统计一下， 就知道这句话可能出现的概率了，但傻子都知道行不通。将`P(S)`展开表示`P(S)=P(w1,w2,...wn)`
4. 利用条件概率的公式，`P(w1,w2,...wn)=P(w1).P(w2|w1).P(w3|w1,w2)...P(wn|w1,w2,...wn-1)`。
5. 可以看到`P(w1)` 比较好算，`P(w3|w1,w2)`已经非常难算了，`P(wn|w1,w2,...wn-1)`的可能性太多，无法估算。为此，俄国数学家马尔科夫提出了一种偷懒但颇为有效地方法：即假设任意一个词wi出现的概率只同它前面的词wi-1 有关，于是公式就可以转换为 `P(w1,w2,...wn)=P(w1).P(w2|w1).P(w3|w2)...P(wn|wn-1)`，对应的统计语言模型叫二元（文法）模型。当然，也可以假设一个词由前面的N-1个词决定，称为N元模型。
6. 现在的问题就变成了 如何计算条件概率 `P(wi|wi-1)`,根据它的定义 `P(wi|wi-1) = P(wi-1,wi)/P(wi-1)`，而估计联合概率`P(wi-1,wi)` 和 `P(wi-1)`，因为有了大量的机读文本（也就是语料库）而变得很简单。只要数一数`wi-1,wi` 这对词在统计文本中前后相邻出现了多少次`#(wi-1,wi)`，以及wi-1本身在同样的文本中出现了多少次`#(w-1)`，然后用两个数分别除以语料库的大小`#`，即可得到词或二元组的相对频度。


        f(wi-1,wi)= #(wi-1,wi) / #
        f(wi-1) = #(wi-1) / #
        // 根据大数定理， 只要统计量足够， 相对频度就等于概率
        P(wi-1,wi) 约等于 #(wi-1,wi) / #
        P(wi-1) 约等于 #(wi-1) / #
        // 因此
        P(wi|wi-1) = P(wi-1,wi)/P(wi-1) = #(wi-1,wi) / #(wi-1) 

7. 经过一系列转换，**P(S) 计算的基本单元变成了统计词和二元组 在语料库中出现的次数**，这是计算机的拿手活。PS：Hadoop的demo 是WordCount 估计就是因为这个

PS: 自然语言是一种上下文相关的信息表达和传递的方式，那自然语言处理模型里一定要体现“上下文相关”的处理思路

### 中文分词

1. 北航的梁南元教授最早提出“查字典”的方法，将句子从左到右扫描一遍，遇到“字典”里有的词就标识出来，遇到复合词就找最长的匹配，遇到不认识的字串就分割成单字词。
2. 哈工大的王晓龙博士将查字典的方法理论化，发展成最少词数的分词理论，即一句话应该分成数量最少的词串
3. 清华大学的郭进博士用统计语言模型成功解决了分词二义性问题，将汉语分词的错误率降低了一个数量级

假定一个句子S可以有几种分词方法，为了简单起见， 假定有以下三种

    A1,A2,A3,...,Ak
    B1,B2,B3,...,Bm
    C1,C2,C3,...,Cn

其中A1,A2,B1,B2,C1,C2 都是汉语的词，以上各种分词结果可能产生不同数量的词串（也就是，有的分词方式分出来的词多，有的分出来的词少），k,m,n 表示不同的数目。那么**最好的一种分词方法应该保证分完词后这个句子出现的概率最大**。也就是说，如果A1,A2,A3,...,Ak 是最好的分词方法，那么其概率应该满足`P(A1,A2,A3,...,Ak) > P(B1,B2,...,Bm)` 并且 `P(A1,A2,A3,...,Ak) > P(C1,C2,...,Cn)`

### 隐含马尔科夫模型

通信的本质是一个编解码和传输的过程，但自然语言处理早期的努力都集中在语法、语义和知识表述上，离通信的原理越走越远，而这样离答案也就越走越远。当自然语言处理的问题回归到通信中的解码问题时，很多难题都迎刃而解了。

在通信中，如何根据接收端的观测信号`o1,o2,o3,...` 来推测信号源发送的信息`s1,s2,s3,...` 呢？只需要从所有的源信息中找到最可能产生观测信号的那一个信息。用概率论的语言来描述， 就是在已知`o1,o2,o3,...`的情况下，求得令条件概率`P(s1,s2,s3,...|o1,o2,o3,...)`达到最大值的那个信息串`s1,s2,s3,...`

感觉需要买本随机过程的书 补一下基础

[【机器学习研究】隐马尔可夫模型 (HMM) 最认真研究](https://cloud.tencent.com/developer/article/1030305) 如何根据海藻的变化推测天气？

## 信息的度量和作用

一本50w字的《史记》有多少信息量， 或者一套莎士比亚全集有多少信息量。我们常说信息有用，那它的作用如何客观、定量的体现出来呢？直到1948年，香农在他著名的论文“通信的数学原理”中提出了信息熵的概念，才解决了信息的度量问题，并且量化出信息的作用。

一条信息的信息量与其不确定性有着直接的关系。比如搞懂一无所知的事情，要了解大量的信息，而如果对某事了解较多，则不需要太多的信息就能把他搞清楚。从这个角度看，信息量就等于不确定性的多少。

## 其它

少年教育

1. 小学生和中学生其实没有必要花那么多时间读书，而他们的社会经验、生活能力以及在那时树立起的指向将帮助他们的一生
2. 中学阶段花很多时间比同伴多读的课程， 上大学以后用很短的时间就能读完，因为在大学阶段，人的理解力要强的多
3. 学习是持续一辈子的过程，若不是出于兴趣，持续学习的动力不足
4. 书本的内容可以早学也可以晚学，但是错过了成长阶段却是无法补回来的

贾里尼克告诉吴军最多的是：什么方法不好。这一点与股神巴菲特给和他吃饭的投资人的建议有异曲同工之处，巴菲特和那些投资人讲，你们都非常聪明，不需要我告诉你们做什么，我只需要告诉你们不要做什么（这样可以少犯很多错误）。这些不要做的事情，是巴菲特从一生的经验教训中得到的。