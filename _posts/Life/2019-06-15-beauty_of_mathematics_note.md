---

layout: post
title: 《数学之美》笔记
category: 生活
tags: Life
keywords: 技术领导力

---

## 简介（持续更新）

* TOC
{:toc}

## 文字、语言、数字、信息 ==> 信息科学规律的引导

大约是公元前32世纪， 埃及象形文字的数量大约只有500个，但到了公元前5-7世纪，象形文字增加到了5000个左右，与中国常用的汉字数量相当。然而随着文明的进步， 信息量的增加，埃及的象形文字数量便不再随着文明的发展而增加了，因为没有人能够学会和记住这么多文字。于是，概念的第一次概括和归类就开始了。比如汉字“日”表示太阳，同时又是太阳从升起到落山再到升起的一天。这种概念的聚类，在原理上与今天自然语言处理或者机器学习的聚类有很大的相似性，只是在远古，这个过程可能需要上千年。

文字出现在远古“信息爆炸”导致人们的头脑装不下这些信息的时候， 那么数字则是出现在人们的财产多到需要数一数才搞清楚有多少的时候。毫无疑问，如果我们有十二个指头，今天我们用的一定是十二进制。当然，算是脚指头有二十进制的，这就是玛雅文明。

阿拉伯数字的革命性不仅在于它的简洁有效， 而且标志着数字和文字的分离。作为对比，中国的个十百千万亿就没有分离。 

从象形文字进化到拼音文字是一个飞跃，因为人类在描述物体的方式上，从物体的外表进化到了抽象的概念。同时不自觉地的采用了对信息的编码。在罗马体系的文字中， 总体来讲，常用字短，生僻字长。而在意型文字（比如中文）中，也是类似，大都常用字笔画少，而生僻字笔画多。这完全符合信息论中的最短编码原理。 这种文字（其实就是编码的方法）带来的好处是书写起来省时间、省材料（纸张要么没有要么不便宜）。

如果说从字母到词的构词法是词的编码规则，那么语法则是语言的编码和解码规则。不过相比较而言，词可以被认为是有限而封闭的集合，而语言则是无限和开放的集合。从数学上讲，对于前者可以有完备的编解码规则，而后者则不具备这个特性。 因此任何语言都有语法规则覆盖不到的地方，这些例外或者说不精确性，让我们的语言丰富多彩（也可以说是病句）。这就涉及到一个语言学研究方法的问题：到底是语言对，还是语法对。前者坚持从真实的语句文本（成为语料）出发，而后者坚持从规则出发，最终自然语言处理的成就宣布了前者的获胜。

语言的出现是为了人类之间的通信。字母（或中文的笔画）、文字和数字实际上是信息编码的不同单位。任何一种语言都是一种编码的方式，而语言的语法规则则是编解码的算法。我们把一个要表达的意思，通过某种语言的一句话表达出来，就是用这种语言的编码方式对头脑中的信息做了一次编码，编码的结果就是一串文字。而如果对方懂得这门语言，他或者她就可以用这门语言的解码方法获得说话人要表达的信息。**这就是语言的数学本质**。 

## 自然语言处理——从规则到统计

自然语言从它产生开始，逐渐演变成一种上下文相关的信息表达和传递的方式。基于规则的方法在“上下文相关”的处理上捉襟见肘。PS：就好比听一个相声，包袱的最后一句话本身并不搞笑，而是全文的铺垫才起到了搞笑的效果。 

从规则到统计，使得，可以且只需用数学的方法给出现今所有自然语言处理相关问题的全部答案。

1. 牛顿：自然哲学的数学原理
2. 香农：通信的数学原理

基于统计的自然语言处理方法， 在数学模型上和通信是相通的，甚至就是相同的。因此，在数学意义上自然语言处理又和语言的初衷——通信联系在一起了。但是，科学家用了几十年才认识到这个联系。

## 统计语言模型

逻辑链条：

1. 统计语言模型产生的初衷是为了解决语音识别问题。在语音识别中，计算机需要知道一个文字序列是否能构成一个大家理解而且有意义的句子， 然后显示或者打印给使用者。
2. 一个句子是否合理，就看它的可能性大小如何，至于可能性就用概率来衡量。PS：在人类说过的所有话中出现的可能性。
3. 假设S表示某一个有意义的句子， 由一连串特定顺序排序的词w1,w2,...wn组成，这里的n是句子的长度。`S=w1,w2,...wn`，S在文本中出现的可能性也就是数学上的概率P(S)，即把人类有史以来讲过的话统计一下， 就知道这句话可能出现的概率了，但傻子都知道行不通。将P(S)展开表示`P(S)=P(w1,w2,...wn)`
4. 利用条件概率的公式，`P(w1,w2,...wn)=P(w1).P(w2|w1).P(w3|w1,w2)...P(wn|w1,w2,...wn-1)`。
5. 可以看到P(w1) 比较好算，P(w3|w1,w2)已经非常难算了，P(wn|w1,w2,...wn-1) 的可能性太多，无法估算。为此，俄国数学家马尔科夫提出了一种偷懒但颇为有效地方法：即假设任意一个词wi出现的概率只同它前面的词wi-1 有关，于是公式就可以转换为 `P(w1,w2,...wn)=P(w1).P(w2|w1).P(w3|w2)...P(wn|wn-1)`，对应的统计语言模型叫二元（文法）模型。当然，也可以假设一个词由前面的N-1个词决定，称为N元模型。
6. 现在的问题就变成了 如何计算条件概率 P(wi|wi-1),根据它的定义 `P(wi|wi-1) = P(wi-1,wi)/P(wi-1)`，而估计联合概率P(wi-1,wi) 和 P(wi-1)，因为有了大量的机读文本（也就是语料库）而变得很简单。只要数一数wi-1,wi 这对词在统计文本中前后相邻出现了多少次`#(wi-1,wi)`，以及wi-1本身在同样的文本中出现了多少次`#(w-1)`，然后用两个数分别除以语料库的大小`#`，即可得到词或二元组的相对频度。


        f(wi-1,wi)= #(wi-1,wi) / #
        f(wi-1) = #(wi-1) / #
        // 根据大数定理， 只要统计量足够， 相对频度就等于概率
        P(wi-1,wi) 约等于 #(wi-1,wi) / #
        P(wi-1) 约等于 #(wi-1) / #
        // 因此
        P(wi|wi-1) = P(wi-1,wi)/P(wi-1) = #(wi-1,wi) / #(wi-1) 
7. 经过一系列转换，P(S) 计算的基本 单元是 统计词和二元组 在语料库中出现的次数，这是计算机的拿手活。PS：Hadoop的demo 是WordCount 估计就是因为这个

PS: 自然语言是一种上下文相关的信息表达和传递的方式，那自然语言处理模型里一定要体现“上下文相关”的处理思路