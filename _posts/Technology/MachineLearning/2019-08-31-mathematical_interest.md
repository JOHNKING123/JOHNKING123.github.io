---

layout: post
title: 机器学习的数学原理
category: 技术
tags: MachineLearning
keywords: 深度学习

---


## 前言（未完成）

## 概率论

![](/public/upload/machine/probability_theory.png)

[为什么我说概率论是大学最不能翘的一门数学课](https://zhuanlan.zhihu.com/p/36920233)

假设你在一个教室里，同时周围有30名学生，游戏的规则是这样的：一开始，大家都是鸡蛋，你们需要和周围同类型的物种进行石头剪子布来升级，鸡蛋找鸡蛋划石头剪子布，胜利者就变成了小鸡。接着小鸡找小鸡划，胜利者就变成了凤凰，然后，凤凰和凤凰划，胜利者就变成了人，变成人的同学就是最后的赢家，同时，每次石头剪子布输的那一方都会降一级，比如从凤凰降级到小鸡，小鸡降级到鸡蛋，游戏限时十分钟，所以，现在问你在这个游戏中如何取胜（此处认真思考二十秒）

如果一个人在这十分钟内只进行一轮石头剪子布（我们现在定义由鸡蛋到人的三次石头剪子布为一轮，如果提前输掉则视作此轮结束）那么这个人变成人的概率为1/8，而如果它进行两轮，那么我们先不管他是在第一轮还是第二轮中变成了人，只要最后变成人了就OK，所以我们只用讨论它两轮都输的概率，就是（1-1/8）^2，而变成人的概率为1-（1-1/8）^2=15/64>1/8，依次类推，可知道如果进行n轮，则他变成人的概率为1-（1-1/8）^n,当n趋于无穷时，这个概率将趋于 1，所以这个游戏取胜的秘诀就是不停的和周围的人进行石头剪子布，频次越高，也就越有机会变成人。

对于大多数玩家而言，是不会仔细思考这个游戏背后的数学模型的。想对上述游戏进行一次完整而严谨的建模也将是一个十分复杂的过程。

[概率：了解不确定性](https://songshuhui.net/archives/93539)在1654年的一天早上，法国数学家布莱兹·帕斯卡收到了他的朋友贡博的一封来信：两位贵族A与B正在进行一场赌局，赌注是每人500法郎，两人轮流掷硬币，得到正面则A得一分，反面则B得一分，每一局两人得分的机会相等，谁先得到6分谁就得到1000法郎。两人激战正酣，比分达到2比4之际，B突然有事需要终止赌局。赌注应该如何分配才最公平。


对于某个非常简单的随机事件，比如说掷硬币，我们知道每种结果出现可能性的大小，这样的事件被称为“基本事件”。我们可以多次重复这些基本事件，假定它们发生的可能性不会改变，而且这些重复没有相互影响。如果我们将这些基本事件以合适的形式组合起来，就能得到一个更为复杂而有趣的系统。许多概率问题实际上就是对这些随机系统的各种性质的研究。

### 概率与自然语言处理

概率的定义就是随机事件发生的可能性的度量，而信息则是减少随机不定性的东西，这两者生而就是有着千丝万缕的联系的，所以我们在研究自然语言处理（本质是通信）这类信息时，也必然要引入概率的知识

### 贝叶斯公式的另类解读


    P(AB) = P(A|B) * P(B) = P(B|A) * P(A)
    P(A|B) = P(B|A) * P(A) / P(B) = P(B|A) / P(B) * P(A)

`P(A|B)`  可以视为 `P(A)` 的增强，也就是后验概率是先验概率的增强。**观测者观测历史数据得出预测假设P(A)，然后新的信息`P(B)`出现，观测者修正预测为`P(A|B)`**。`P(B|A) / P(B)` 被称为调整因子。

定义P(A|B)是P(B|A)的逆概率。贝叶斯最早的目的就是研究一个概率和他的逆概率之间的关系。

在实际的场景下， A 和 B 通常代表了一个结果和原因， `P(结果|原因)`好算，`P(原因|结果)`知果索因就很麻烦，**不然侦探片就没那么好看了**。

人们已经能够计算”正向概率“，如“一个袋子N个白球M个黑球，你伸手进去摸一把，摸出黑球的概率多大”。而一个自然而然的问题反过来：如果事先不知道袋子中黑白球比例，而是闭着眼睛摸出好几个球，观察这些取出来的球的颜色之后，那么我们可以对袋子中黑白球的比例做出怎样的推测？如果再摸出几个球，是否要对刚才的推测进行校正？

贝叶斯公式看着没什么感觉，但在贝叶斯分类中就很有用武之地了。比如我们将水果的形状、颜色、纹理、重量、握感、口感全部数值化。可以得到一个苹果、橙子是黄颜色的概率分别是多少，基于贝叶斯公式就可以反推一个黄色的、圆形水果是苹果、橙子的概率。前者是训练数据，后者是计算机根据贝叶斯分类算法做出的判断。

[怎么简单理解贝叶斯公式?半瓶晃荡加水中的回答 ](https://www.zhihu.com/question/51448623/answer/306116102)最重要的思维并非是逻辑思维，是直觉式思维。在学习过程中，必须把知识和直觉思维联系起来，才能真正掌握这个知识。直觉性思维的本质，是通过观察少量结果，反向推导出事物的因果联系。

### 概率论与数理统计

||概率论|数理统计|
|---|---|---|
|方法|根据已知的分布来分析随机变量的特征与规律|根据得到的观察结果对原始分布做出推断|
|例子|已知摇奖规律判断一注号码中奖的可能性|根据之前多次中奖不中奖的号码以一定的精确性推断摇奖的规律|
|内部争论|分布是确定的，但函数参数有待确认<br>最大似然法：使训练数据出现的概率最大化 ==> 进而确定未知参数<br>最大后验概率法：使未知参数出现的可能性最大化，选取最可能的未知参数取值作为估计值||


## 逻辑回归/Logistic regression

[对线性回归，logistic回归和一般回归的认识](https://www.cnblogs.com/jerrylead/archive/2011/03/05/1971867.html)

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

线性回归假设特征和结果满足线性关系，线性关系的表达能力非常强大，每个特征对结果的影响强弱可以由前面的参数体现，

首先给出一个输入数据，我们的算法会通过一系列的过程得到一个估计的函数，这个函数有能力对没有见过的新数据给出一个新的估计，也被称为构建一个模型。

估计函数：$$h=w\_1x\_1+w\_2x\_2+b$$

我们程序也需要一个机制去评估\\(\theta=(w1,w2,b)\\)是否比较好，一般这个函数称为损失函数（loss function），描述估计函数不好的程度


$$J(\theta)=\frac{1}{2}\sum_{i=1}^{m}(h^i-y^i)^2$$

只需要确定参数(w1,w2,b)，就可以将模型用来预测。然而(w1,w2,b)需要在J(w1,w2,b)最小的情况下才能确定。因此问题归结为求极小值问题，求解极小值 有最小二乘法和梯度下降法

### 最小二乘法 ==> 求使得J极小的(w1,w2,b)

将训练特征表示为X矩阵，结果表示成Y向量

$$\theta=(X^TX)^{-1}X^TY$$

### 梯度下降法 ==> 求使得J极小的(w1,w2,b)

1. 首先对(w1,w2,b)赋值，这个值可以是随机的，也可以让(w1,w2,b)是一个全零的向量
2. 改变(w1,w2,b)的值，使得J(w1,w2,b)按梯度下降的方向进行减少

梯度方向由J(w1,w2,b)对(w1,w2,b)的偏导数确定

### 为什么使用平方和作为loss function

假设根据特征的预测结果与实际结果有误差Ei，那么预测结果hi和真实结果yi满足下式：

$$y^i=h^i+E^i$$

也就是

$$y^i=\theta^T X^i+E^i$$

一般来讲，误差满足平均值为0的高斯分布，也就是正态分布
 
 $$p(y^i|x^i;\theta)= \frac{1}{\sqrt{2\pi}\sigma}e^\frac{(y^i-h^i)^2}{2\sigma^2}$$
 

这样就估计了一条样本的结果概率，然而我们期待的是模型能够在全部样本上预测最准，也就是概率积最大。注意这里的概率积是概率密度函数积，连续函数的概率密度函数与离散值的概率函数不同。这个概率积成为最大似然估计。我们希望在最大似然估计得到最大值时确定(w1,w2,b)。那么需要对最大似然估计公式求导，求导结果即是

$$\frac{1}{2}\sum_{i=1}^{m}(y^i-\theta^T X^i)^2$$

### 几何意义 vs 概率意义

《裂变：秒懂人工智能基础课》：对于单变量线性回归而言，在误差函数服从正态分布的情况下，从几何意义（找到一条线将空间分为两个部分）出发的最小二乘法（使均方误差取得最小值为目标的模型求解方法）与从概率意义出发的最大似然估计是等价的