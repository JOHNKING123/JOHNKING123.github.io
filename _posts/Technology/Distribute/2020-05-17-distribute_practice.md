---

layout: post
title: 分布式系统实战
category: 技术
tags: Distribute
keywords: 分布式系统

---

## 简介（未完成）

* TOC
{:toc}


## InfluxDB 企业版的架构

InfluxDB 企业版是由 META 节点和 DATA 节点 2 个逻辑单元组成的
1. META 节点存放的是系统运行的关键元信息，比如数据库（Database）、表（Measurement）、保留策略（Retention policy）等。它的特点是一致性敏感，但读写访问量不高，需要一定的容错能力。
2. DATA 节点存放的是具体的时序数据。它有这样几个特点：最终一致性、面向业务、性能越高越好，除了容错，还需要实现水平扩展，扩展集群的读写性能。

**对于 META 节点来说，节点数的多少代表的是容错能力**，一般 3 个节点就可以了，因为从实际系统运行观察看，能容忍一个节点故障就可以了。但**对 DATA 节点而言，节点数的多少则代表了读写性能**，一般而言，在一定数量以内（比如 10 个节点）越多越好，因为节点数越多，读写性能也越高，但节点数量太多也不行，因为查询时就会出现访问节点数过多而延迟大的问题。

1. META 节点需要强一致性，实现 CAP 中的 CP 模型。使用 Raft 算法实现 META 节点的一致性（一般推荐 3 节点的集群配置）
2. DATA 节点存放的是具体的时序数据，对一致性要求不高，实现最终一致性就可以了。但是，DATA 节点也在同时作为接入层直接面向业务，考虑到时序数据的量很大，要实现水平扩展，所以必须要选用 CAP 中的 AP 模型，因为 **AP 模型不像 CP 模型那样采用一个算法（比如 Raft 算法）就可以实现了**

也就是说，AP 模型更复杂，具体有这样几个实现步骤。
1. 自定义副本数
2. Hinted-handoff。一个节点接收到写请求时，需要将写请求中的数据转发一份到其他副本所在的节点，那么在这个过程中，远程 RPC 通讯是可能会失败的，比如网络不通了、目标节点宕机了、临时的突发流量也会导致系统过载。那么如何处理这种情况呢？答案是实现 Hinted-handoff。在 InfluxDB 企业版中，Hinted-handoff 是这样实现的:
    1. 写失败的请求，会缓存到本地硬盘上 ;
    2. 周期性地尝试重传 ;
    3. 相关参数信息，比如缓存空间大小 (max-szie)、缓存周期（max-age）、尝试间隔（retry-interval）等，是可配置的。

    虽然 Hinted-handoff 可以通过重传的方式来处理数据不一致的问题，但当写失败请求的数据大于本地缓存空间时，比如某个节点长期故障，写请求的数据还是会丢失的，最终的节点的数据还是不一致的，那么怎么实现数据的最终一致性呢？答案是反熵。
3. 反熵。一种异步修复、实现最终一致性的协议


## 实现

[hashicorp/raft](https://github.com/hashicorp/raft)

## 其它

在海量系统中建议直面问题，通过技术手段在代码和架构层面解决它，而不是引入和堆砌更多的开源软件。