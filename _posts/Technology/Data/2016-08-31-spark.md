---

layout: post
title: Spark初识
category: 技术
tags: Data
keywords: Spark

---

## 前言 

## 分布式计算



spark 用于对 应用进行调度、分发及监控。 这里的应用由多个计算任务组成，运行在多个机器上。具体的说

1. 任务调度，启动进程，启动其它主机的进程，来运行任务，并感知它们
2. 定义进程的任务：接收、汇报（通用的） + 数据处理
3. 数据处理的中间结果，对于单机则一直在内存中，对于分布式则要在主机之间流转

大数据跟迭代的密切关系，大数据一般是大量重复schema的数据。

mesos 提供了基本接口之后，上层可以运行 各种framework，比如marathon。spark 提供了rdd 等基本接口后，根据业务属性的不同，上层可以运行 各种frame work，比如spark stream等

## 从操作上直观感受 spark

单机模式

1. 官网下载spark-2.3.0-bin-hadoop2.7.tgz包，解压
2. 启动master，`spark-2.3.0-bin-hadoop2.7/sbin/start-master.sh`
3. 可以在 `http://localhost:8080` 下查看 webui
4. 启动一个slave，`spark-2.3.0-bin-hadoop2.7/sbin/start-slave.sh spark://localhost:7077`

![](/public/upload/data/spark_ui.png)

从启动过程及web ui 上可以看到

1. 跟mesos 很像，mesos 也是start-master,start-slave
1. 启动一个slave后，worker 列表新增了一行元素，包括了地址、状态、cpu核数及内存。跟mesos 的agent 列表如出一辙
2. rdd 跟 java8 的stream 很像，比如惰性求值等。前者将操作 扩展到多个机器上，进而我们可以类比，stream 的实现forkjoin 和 rdd 的实现的异同。


## 交互式查询

传统的 通过提交代码与 spark 或 mapreduce 交互

一个mapreduce 任务执行的流程

1. 编写代码
2. 打成jar 包
3. hadoop master 机器上 `hadoop jar wordcount.jar input_arg output_arg`

对应到 spark 则是

1. 编写代码
2. `bin/spark-submit --class xx.xx.wordcount target_jar input_arg out_arg`

两者本质上都差不多，都是将任务 提交给 spark master，master 分派slave，命令slave 准备数据， 将jar 分发到 各个slave 上执行，slave 执行完毕后，master 视情况汇总结果等。

《Spark快速大数据分析》讲到spark shell时提到：使用其它shell工具，你只能用单机的硬盘和内存来操作数据，而Spark Shell可用来与分布式存储在许多机器的内存或者硬盘上的数据进行交互，并且处理过程的分发由spark自动控制完成（spark 速度快，速度快就意味着我们可以进行 交互式的数据操作）。

	scala> val input = sc.textFile("/tmp/inputFile")
	spark info...
	scala> val words = input.flatMap(line => line.split(" "))
	spark info...
	scala> val counts = words.map(word => (word,1)).reduceByKey{case (x,y) => x+y}
	spark info...
	scala> counts.saveAsTextFile("/tmp/output")
	spark info...

交互式查询体现了，函数级的filter、map等操作 也可以在 集群中 并行执行。从slave的视角看，如果是mapreduce，上述四个操作一气呵成，必须执行完毕后才可以等待master 进一步指示。

此处，运行完毕后，`/tmp/output` 是一个目录，其结构 跟 hdfs 是一样一样的，这是saveAsTextFile 选择的策略。

## rdd——spark和MR的最大不同

说一些官话的话，MR的不足如下

1. 复杂的计算需要大量的job完成
2. 依赖关系是由开发者自己管理的
3. 没有整理逻辑
4. 中间结果只能放在HDFS中
5. 对于实时数据处理的支持不够

我的一个感觉是，MapReduce对数据的处理由一整套自己的流程，中间Map和Reduce逻辑留给开发人员。这从它留给开发人员的接口可以看出来，Map和Reduce是一个个类，它们是一个复杂执行流程的一环。

数据处理的中间结果，因为要在主机之间流转，hadoop的选择是，将其搞成文件，而内存数据搞成文件后，通常又以行来划分。

而RDD无需文件来存储中间结果，所以hadoop操作和RDD有所不同，RDD的形式可以更丰富。

1. rdd 支持两种操作：转化操作和行动操作
2. 惰性求值，在 行动操作开始之前，spark 不会开始 转换操作。
3. 转化操作 会返回一个新的rdd，老的rdd 数据不会被改变
4. rdd 根据转换操作 形成lineage graph，每当调用一个行动操作，lineage graph 都会从头开始计算

||存储|提供操作|
|---|----|----|
|mapreduce|hdfs|map/reduce|
|sparck|RDD|transform,actiion|

spark 更像是用户事先定义好：rdd从哪里来，到哪里去，数据如何变化。spark会对lineage graph进行类似sql语义的解析，然后**融合数据的读取、转换和处理流程**。具体的说，数据库对外提供的抽象（或使用方式）是sql，在从物件文件读取数据前，先对sql进行完整的解析，先读哪，再读哪，是否有缓存，是否查索引，最终制定一个执行计划。spark对外的抽象是rdd代码，最终转化为rdd lineage graph，读取数据的时候，就可以决定哪一步可以过滤掉。sql可以向数据库读写数据，rdd也可以。**而hadoop因为是过程式的**，数据的处理和数据的读写、中转是不相关的。哪怕后面的逻辑表明一半数据是废掉的，读取时依然要先全部读取到内存。

## spark 和strorm

storm做的是流式处理，就是数据不停地来，storm一直在运行，就是kafka、rabbit可以不停地接消息，storm可以不停地处理消息一样。

hadoop和spark等则是批处理，其数据源是一个明确的文件，输出也是一个名明确的文件。

（待整理）实时性和大数据量其实是不可兼得的，比如数据量很大的话，就要移动计算（而不是移动数据），这时实现实时性就比较困难。


## spark 和marathon

mesos上，提供framework的抽象，可以运行很多分布式任务，大部分是短暂的，也有像marathon这种，长时间任务。

从marathon framework的角度将spark和marathon做一个对比

||基于的运行环境|提供抽象|一点不同|
|---|---|---|---|---|
|matahon|mesos|app、deployment等|marathon是一个long-live framework，可以管理app|
|spark|mesos/yarn等|rdd|spark 谈不上是一个framework，更像是一个framework的半成品（提供基础、抽象），结合用户代码作为一个framework执行，是否是long-live由用户对spark组件的选择决定||

从程序的执行上，spark和marathon挺一样的，或者说分布式程序都挺一样的。

1. 首先有一个正在运行的集群管理器
2. spark、marathon等作为framework，配置集群管理器（yarn/mesos等）的地址（以便与集群管理其交互），作为一个独立的进程执行。假设我们现在在开发机上有一个可执行jar，我们执行这个jar，那么spark这个framework的主进程则运行在本机，通过与集群管理器安排framework的执行进程在集群中运行。

对于spark（其它framework也一样）来说，还可以将jar（及其依赖的jar）提交到集群管理器上，由集群管理器驱动framework主进程及执行进程的执行。

## spark stream（未完成） 

spark stream 是微批处理。

spark，数据流进来，根据时间分隔成一小段，一小段做处理1、处理2、处理3。
storm，一个消息进来，一个消息做处理1、处理2、处理3


