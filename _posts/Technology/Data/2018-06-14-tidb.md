---

layout: post
title: TIDB 学习
category: 技术
tags: Data
keywords: TIDB

---

## 前言（持续更新）

[源码](https://github.com/pingcap/tidb)TiDB ("Ti" stands for Titanium) is an open-source NewSQL database that supports Hybrid Transactional and Analytical Processing (HTAP) workloads. It is MySQL compatible and features horizontal scalability, strong consistency, and high availability. [TiDB 源码阅读系列文章（一）序](https://zhuanlan.zhihu.com/p/34109413)

Ti表示Titanium钛

![](/public/upload/data/tidb_xmind.png)

## 基本理念

[势高，则围广：TiDB 的架构演进哲学](https://zhuanlan.zhihu.com/p/67552966)未读

[TiDB 架构的演进和开发哲学](https://zhuanlan.zhihu.com/p/25142743)

1. 所有计算机科学里面的问题都可以把它不停地抽象，抽象到另外一个层次上去解决。
2. 我们在做技术选型的时候，如果在有很大自由度的前提下，怎么去控制发挥欲望和膨胀的野心？你的敌人并不是预算，而是复杂度。 这一点跟 [《从0开始学架构》笔记](http://qiankunli.github.io/2018/05/06/architecture_from_0_note.html) 异曲同工。
3. 你怎么控制每一层的复杂度是非常重要的，特别是对于一个架构师来说，所有的工作都是在去规避复杂度，提升开发效率和稳定性。
4. 我仔细看过 Etcd 的源码，每个状态的切换都抽象成接口，我们测试是可以脱离整个网络、脱离整个 IO、脱离整个硬件的环境去构建的。我觉得这个思路非常赞
5. 对架构师而言一个很重要的工作就是查看系统中有哪些 block 的点，挨个解决掉这些问题。所有的东西，只要有 Metrices，能被监控，这个东西就能被解决。

[十问 TiDB ：关于架构设计的一些思考](https://zhuanlan.zhihu.com/p/38254903)**这个世界有很多人，感觉大于思想，疑问多于答案。**

## SQL存储易变，SQL协议长存

笔者在学习Redis时，看到Redis 通信协议，除了Redis 一些基于磁盘的KV 存储也在使用Redis 协议与客户端交互，比如[Qihoo360/pika](https://github.com/Qihoo360/pika)，一个大牛提到：Redis 后端易变，Redis协议/规范长存。

相比Redis规范，SQL 则更是老当益壮的典范，早就超脱了RDBMS的范畴

1. RDBMS，用SQL 表达对磁盘的存取逻辑
1. Hive， 用SQL 编写逻辑来代替 mapreduce， 读取 hdfs 文件
2. Spark SQL，用SQL 编写逻辑来表达spark 计算，进行大数据计算
3. 用SQL 编写逻辑来表达flink 计算，进行实时计算
4. 业界的一些通用平台，用SQL来表达逻辑，进行跨hdfs、mysql等多存储系统的 计算
5. TiDB，用SQL 表达对TiKV的存取逻辑

这些系统的通用架构 都是

1. 接口层
2. SQL 解释层，将SQL 转换为对应底层系统的逻辑计算
3. 系统本身的逻辑 + 存储层

## 整体设计

[畅想TiDB应用场景和HTAP演进之路](https://blog.bcmeng.com/post/tidb-application-htap.html#5-tidb-htap-%E6%BC%94%E8%BF%9B%E4%B9%8B%E8%B7%AF)

![](/public/upload/data/tidb_overview.png)

从下往上看

1. TiKV Server， pingcap 将其独立的作为 一个源码库 [tikv/tikv](https://github.com/tikv/tikv)，TiKV 是一个**支持事务的、分布式、Key-Value、存储引擎**。如果不考虑Region复制，一致性，和事务的话，TiKV其实和HBase很像，底层数据结构都是LSM-Tree, Region都是Range分区, Region都是计算和负载均衡的基本单位。
2. TiDB Server 负责接收 SQL 请求，生成SQL的逻辑计划和物理计划，并通过 PD 找到存储计算所需数据的 TiKV 地址，将SQL转换为TiKV的KV操作，与 TiKV 交互获取数据，最终返回结果。TiDB Server 是无状态的，其本身并不存储数据，只负责计算，可以无限水平扩展。 
3. Placement Driver 主要有以下职责：

    1. 集群的元信息 （某个 Key 存储在哪个 TiKV 节点）
    2. TiKV 集群进行调度和负载均衡
    3. 分配全局唯一且递增的事务 ID

## 存储——TIKV

好玩的是，饿了么基于TiKV 构建统一KV系统，TiKV 之上增加了redis协议

### 代码访问

基于golang的代码访问示例

    func main() {
        cli, err := tikv.NewRawKVClient([]string{"192.168.199.113:2379"}, config.Security{})
        key := []byte("Company")
        val := []byte("PingCAP")
        // put key into tikv
        err = cli.Put(key, val)
        // get key from tikv
        val, err = cli.Get(key)
        // delete key from tikv
        err = cli.Delete(key)
        fmt.Printf("key: %s deleted\n", key)
        // get key again from tikv
        val, err = cli.Get(key)
        fmt.Printf("found val: %s for key: %s\n", val, key)
    }

支持事务

    Begin() -> Txn
    Txn.Get(key []byte) -> (value []byte)
    Txn.Set(key []byte, value []byte)
    Txn.Iter(begin, end []byte) -> Iterator
    Txn.Delete(key []byte)
    Txn.Commit()


### 实现原理

[PingCAP公司博客](https://pingcap.com/blog-cn/)

[三篇文章了解 TiDB 技术内幕——说存储](https://zhuanlan.zhihu.com/p/26967545)

1. 单机存储使用了rocksdb，本质上还是kv 存储
2. 数据的写入是通过 Raft 这一层的接口写入，通过 Raft将“log”复制到多台机器上，而不是直接写 RocksDB。

![](/public/upload/data/tikv_xmind.png)


### 数据分片

[带着问题学习分布式系统之数据分片](https://www.cnblogs.com/xybaby/p/7076731.html)

1. 分片的考量

	* 如何划分
	* 数据规模变大时，是否可以通过新增节点来动态适应
	* 当某个节点故障的时候，能否将该节点上的任务均衡的分摊到其他节点
	* 对于可修改的数据（比如数据库数据），如果某节点数据量变大，能否以及如何将部分数据迁移到其他负载较小的节点，及达到动态均衡的效果？
	* 元数据的管理（即数据与物理节点的对应关系）规模？元数据更新的频率以及复杂度？
2. 分片的几种方式

	* hash
	* consistent hash without virtual node
	* consistent hash with virtual node
	* range based。

		* 假设以id 作为分片特征值，那么一个节点可能负责0~100,300~400,800~900等，元数据服务记录range与节点的映射关系
		* 区间的大小不是固定的，以数据量的大小为片段标准。即0~100占了1M，100~150 也可能占了1M

	对于range based 来说，如果一个节点负责的数据只有一个区间，range based与没有虚拟节点概念的一致性hash很类似；如果一个节点负责多个区间，range based与有虚拟节点概念的一致性hash很类似。
3. 分片特征值的选择
4. 分片元数据及元数据服务

## SQL 层

[三篇文章了解 TiDB 技术内幕——说计算](https://zhuanlan.zhihu.com/p/27108657)未读

## 调度

[三篇文章了解 TiDB 技术内幕 —— 谈调度](https://zhuanlan.zhihu.com/p/27275483)未读