---

layout: post
title: 推送系统的几个基本问题
category: 技术
tags: Architecture
keywords: push

---

## 简介（未完成）

## 功能问题


1. 推送系统和推送服务商关系
	
	推送服务商只是实现了到设备的可达性，什么时候和发什么，仍有推送系统决定

2. 推送系统日志采集与问题排查
	* 唯一id关联推送的各个阶段
	* 排查需求：某人为什么没收到消息

	所以日志中应该包括：时间，用户，消息三个基本信息
	
	* 统计需求：每天的发送、接收和点击；一个全局推、推广推送的发送、接收和点击

3. 通道选择
	
	* 客户端的轮询是否也可以作为一个通道（跟现有的消息中心类似）
	* 长连接通道（针对即时性要求较高的业务）
	* 推送服务商通道

4. 推送过滤

	* 根据用户设置、频控、用户打开时间
	* 计算用户对推送内容的兴趣程度决定是否推送
	* 两个互斥的业务

	第一个是变化不大的，后两个则每天都会变化，如何动态插拔过滤规则
	
5. 送达数和点击数的利用

	* 分析用户兴趣
	* 评估推送效果
6. 面向使用者接口

	* 关键就是消息model的定义。消息id，基本内容，客户端行为

7. 客户端的准备

	* 各个通道，统一消息model的处理

8. 推送对其它系统的作用

	* 提高用户对业务的粘性

9. 各个系统之间的数据格式
10. 优化设计

	* 消息调度

11. 推送需求

	* 一般业务通知，与用户相关。比如“比如您订阅的xx更新了”
	* 平台推广/用户唤醒，一个用户一天至少收到一次。全局推送 + 个性化推送
	* 部分用户群体，特定信息通知。

	
## 整体设计

第一代：业务方 ==> token 转换 == 用户设置、频控、亲密度等过滤 ==> 多通道推送。中间使用消息队列 勾连

问题：出现大v的相关事件推送时，会堵。在该方案下，任何优化（包括批量、动态调度）都只是小有提高，无法从根本上解决问题。

可以优化的地方：

1. 根据 uid 查询 推送token 要经过mysql
2. 查询用户设置时，推送系统 ==> rpc ==> 用户设置服务 ==> 用户设置db，较为耗时。频控、亲密度等类似。
3. 推送系统 提供的接口 为 `<uid/uid list,msg>`，对于特定业务，比如像大v的所有粉丝发推送，业务需要将大v的所有粉丝查出来，调用较为麻烦。

一个设想就是，所有的数据都在 db、redis、hdfs 中，与其实现rpc 服务封装它们，不如直接访问这些数据，筛选出结果 交给 通道服务器直接推送。


## 性能问题

1. 全局推和个性化推送的冲突问题

	* 全局推顺序查询数据库，个性化推送随机查询数据库
	* 全局推和个性化推送量都比较大，同时进行时，系统不堪重负