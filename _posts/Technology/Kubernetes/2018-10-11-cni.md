---

layout: post
title: CNI
category: 技术
tags: Kubernetes
keywords: CNI Kubernetes

---


## 简介

* TOC
{:toc}

## 整体架构

### 整体思路

[理解 CNI 和 CNI 插件](https://mp.weixin.qq.com/s/g3QECjZOgbEZ8FG9R3b9iw)CNI 插件的实现通常包含两个部分：

1. 一个二进制的 CNI 插件去配置 Pod 网卡和 IP 地址。这一步配置完成之后相当于给 Pod 上插上了一条网线，就是说它已经有自己的 IP、有自己的网卡了；

    ![](/public/upload/network/cni_set_pod_ip.png)

    通常我们会用一个 "veth" 这种虚拟网卡，一端放到 Pod 的网络空间，一端放到主机的网络空间，这样就实现了 **Pod 与主机这两个命名空间的打通**。

2. 一个 Daemon 进程去管理 Pod 之间的网络打通。这一步相当于说将 Pod 真正连上网络，让 Pod 之间能够互相通信。

    1. 首先 CNI 在每个节点上运行的 Daemon 进程会学习到集群所有 Pod 的 IP 地址及其所在节点信息；学习的方式通常是通过监听 K8s APIServer，拿到现有 Pod 的 IP 地址以及节点，并且新的节点和新的 Pod 的创建的时候也能通知到每个 Daemon。
    2. 拿到 Pod 以及 Node 的相关信息之后，再去配置网络进行打通。首先 Daemon 会创建到**整个集群所有节点的通道**。这里的通道是个抽象概念，具体实现一般是通过 Overlay 隧道、阿里云上的 VPC 路由表、或者是自己机房里的 BGP 路由完成的。第二步是**将所有 Pod 的 IP 地址跟上一步创建的通道关联起来**。关联也是个抽象概念，具体的实现通常是通过 Linux 路由、fdb 转发表或者OVS 流表等完成的。**Linux 路由可以设定某一个 IP 地址路由到哪个节点上去**。fdb 转发表是 forwarding database 的缩写，就是把某个 Pod 的 IP 转发到某一个节点的隧道端点上去（Overlay 网络）。OVS 流表是由 Open vSwitch 实现的，它可以把 Pod 的 IP 转发到对应的节点上。

### CNI SPEC

CNI (Container Network Interface): Specification that act as interface between Container runtime and networking model implementations。

The cni specification is lightweight; it only deals with the network connectivity of containers,as well as the garbage collection of resources once containers are deleted.

![](/public/upload/docker/cni_3.png)

cni 接口规范，不是很长[Container Network Interface Specification](https://github.com/containernetworking/cni/blob/master/SPEC.md)（原来技术的世界里很多规范用Specification 来描述）。对 CNI SPEC 的解读 [Understanding CNI (Container Networking Interface)](http://www.dasblinkenlichten.com/understanding-cni-container-networking-interface/)

kubernetes 对CNI 的实现（**SPEC复杂的描述体现在 code 上就是几个函数**）

    ## github.com/containernetworking/cni/libcni/api.go
    type CNI interface {
        AddNetworkList(net *NetworkConfigList, rt *RuntimeConf) (types.Result, error)
        DelNetworkList(net *NetworkConfigList, rt *RuntimeConf) error
        AddNetwork(net *NetworkConfig, rt *RuntimeConf) (types.Result, error)
        DelNetwork(net *NetworkConfig, rt *RuntimeConf) error
    }

wiki 在描述驱动系统时提到：**A driver provides a software interface to hardware devices**, enabling operating systems and other computer programs to access hardware functions without needing to know precise details about the hardware being used. CNI interface 将一个复杂的系统收敛到几个“入口”，运用之妙存乎一心。

## "裸机" 使用cni

[Understanding CNI (Container Networking Interface)](http://www.dasblinkenlichten.com/understanding-cni-container-networking-interface/)

	mkdir cni
	user@ubuntu-1:~$ cd cni
	user@ubuntu-1:~/cni$ curl -O -L https://github.com/containernetworking/cni/releases/download/v0.4.0/cni-amd64-v0.4.0.tgz
	user@ubuntu-1:~/cni$ tar -xzvf cni-amd64-v0.4.0.tgz
	user@ubuntu-1:~/cni$ ls
	bridge  cni-amd64-v0.4.0.tgz  cnitool  dhcp  flannel  host-local  ipvlan  loopback  macvlan  noop  ptp  tuning

创建一个命名空间

	sudo ip netns add 1234567890

调用cni plugin将 container（也就是network namespace） ADD 到 network 上

	cat > mybridge.conf <<"EOF"
	{
	    "cniVersion": "0.2.0",
	    "name": "mybridge",
	    "type": "bridge",
	    "bridge": "cni_bridge0",
	    "isGateway": true,
	    "ipMasq": true,
	    "ipam": {
	        "type": "host-local",
	        "subnet": "10.15.20.0/24",
	        "routes": [
	            { "dst": "0.0.0.0/0" },
	            { "dst": "1.1.1.1/32", "gw":"10.15.20.1"}
	        ]
	    }
	}
	EOF

	sudo CNI_COMMAND=ADD CNI_CONTAINERID=1234567890 CNI_NETNS=/var/run/netns/1234567890 CNI_IFNAME=eth12 CNI_PATH=`pwd` ./bridge < mybridge.conf
	
mybridge.conf 描述了network 名为mybridge的配置，然后查看1234567890 network namespace 配置

 	sudo ip netns exec 1234567890 ifconfig
	eth12     Link encap:Ethernet  HWaddr 0a:58:0a:0f:14:02
	          inet addr:10.15.20.2  Bcast:0.0.0.0  Mask:255.255.255.0
	          inet6 addr: fe80::d861:8ff:fe46:33ac/64 Scope:Link
	          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
	          RX packets:16 errors:0 dropped:0 overruns:0 frame:0
	          TX packets:8 errors:0 dropped:0 overruns:0 carrier:0
	          collisions:0 txqueuelen:0
	          RX bytes:1296 (1.2 KB)  TX bytes:648 (648.0 B)
	 
	user@ubuntu-1:~/cni$ sudo ip netns exec 1234567890 ip route
	default via 10.15.20.1 dev eth12
	1.1.1.1 via 10.15.20.1 dev eth12
	10.15.20.0/24 dev eth12  proto kernel  scope link  src 10.15.20.2

这个例子并没有什么实际的价值，但将“cni plugin操作 network namespace” 从cni繁杂的上下文中抽取出来，让我们看到它最本来的样子。**从这也可以看到，前文画了图，整理了脑图，但资料看再多，都不如实操案例来的深刻。才能不仅让你“理性”懂了，也能让你“感性”懂了**

## Using CNI with container runtime

[Using CNI with Docker](http://www.dasblinkenlichten.com/using-cni-docker/) net=none 创建的容器：`sudo docker run --name cnitest --net=none -d jonlangemak/web_server_1`，为其配置网络 与 上文的为 network namespace 配置网络是一样的。 

I mentioned above that rkt implements CNI. In other words, rkt uses CNI to configure a containers network interface.

![](/public/upload/docker/rocket_cni.png)

1. network 要有一个json 文件描述，这个文件描述 放在rkt 可以识别的`/etc/rkt/net.d/` 目录下
2. ` sudo rkt run --interactive --net=customrktbridge quay.io/coreos/alpine-sh` 便可以创建 使用customrktbridge network 的容器了。类似的，是不是可以推断`docker network create` 便是 将 network json 文件写入到相应目录下
3. 表面上的`sudo rkt run --interactive --net=customrktbridge quay.io/coreos/alpine-sh` 关于网络部分 实际上 是 `sudo CNI_COMMAND=ADD CNI_CONTAINERID=1234567890 CNI_NETNS=/var/run/netns/1234567890 CNI_IFNAME=eth12 CNI_PATH=pwd ./bridge < mybridge.conf
` 执行，要完成这样的“映射”，需要规范定义 以及 规范相关方的协作，可以从这个角度再来审视前文对CNI SPEC 的一些梳理。


笔者以前一直有一个困惑，network、volume 可以作为一个“资源”随意配置，可以是一个json的存在，尤其是network，`docker network create ` 完了之后 就可以在`docker run -net=xx` 的时候使用。kubernetes 中更是 yaml 中声明一下network即可使用，是如何的背景支撑这样做？ 结合源码来看 [加载 CNI plugin](http://qiankunli.github.io/2018/12/31/kubernetes_source_kubelet.html) Kubelet 会根据 network.json `cmd:=exec.Command(ctx,"bridge");cmd.Run()`

	{
	    "cniVersion": "0.2.0",
	    "name": "mybridge",
	    "type": "bridge",
	    "bridge": "cni_bridge0",
	    "isGateway": true,
	    "ipMasq": true,
	    "ipam": {
	        "type": "host-local",
	        "subnet": "10.15.20.0/24",
	        "routes": [
	            { "dst": "0.0.0.0/0" },
	            { "dst": "1.1.1.1/32", "gw":"10.15.20.1"}
	        ]
	    }
	}

答案就在于：我们习惯了主体 ==> 客体，比如docker早期版本，直接docker ==> container/network namespace。 而cni体系中则是runtime ==> cni plugin ==> container/network namespace。container runtime看作是一个network.json文件的“执行器”，通过json 文件找到cni plugin binary 并驱动其执行。一个network 不是一个真实实体，netowrk.json描述的不是如何创建一个网络，而是描述了如何给一个container 配置网络。


## Using CNI with CRI

在 Kubernetes 中，处理容器网络相关的逻辑并不会在kubelet 主干代码里执行，而是会在具体的 CRI（CContainer Runtime Interface，容器运行时接口）实现里完成。对于 Docker 项目来说，它的CRI 实现叫作 dockershim

## container networking stack

|分层|包括哪些|作用|
|---|---|---|
|the low-level networking layer|networking gear(网络设备),iptables,routing,ipvlan,linux namespaces|这些技术已经存在很多年，我们只是对它们的使用|
|the container networking layer|single-host bridge mode,multi-host,ip-per-container|对底层技术provide some abstractions|
|the container orchestration layer|service discovery,loadbalance,cni,kubernetes networking|marrying the container scheduler's decisions on where to place a container with the primitives provided by lower layers. |

![](/public/upload/docker/container_networking.png)



